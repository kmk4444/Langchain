{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNM5Qu7lfA15m6+AbZnYM0E",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kmk4444/Langchain/blob/main/Chain.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Langchain - Chains**\n",
        "- In Langchain, there are a lot of chains but we will examine two chains.\n",
        "- First chain relates on RAG.\n",
        "- Second chain relates on Function Calling which is answering our function."
      ],
      "metadata": {
        "id": "iU3vG17NV-zZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Requirements.txt**"
      ],
      "metadata": {
        "id": "WK_Q-1CCXWdK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!touch requirements.txt\n",
        "!echo langchain >> requirements.txt\n",
        "!echo langchain-openai >> requirements.txt\n",
        "!echo langchain-google-genai >> requirements.txt\n",
        "!echo langchain_experimental >> requirements.txt\n",
        "!echo openai >> requirements.txt\n",
        "!echo anthropic >> requirements.txt\n",
        "!echo cohere >> requirements.txt\n",
        "!echo streamlit >> requirements.txt\n",
        "!echo python-dotenv >> requirements.txt\n",
        "!echo beautifulsoup4 >> requirements.txt\n",
        "!echo faiss-cpu >> requirements.txt\n",
        "!echo pypdf >> requirements.txt\n",
        "!echo unstructured >> requirements.txt\n",
        "!echo networkx >> requirements.txt\n",
        "!echo openpyxl >> requirements.txt\n",
        "!echo rapidocr-onnxruntime >> requirements.txt"
      ],
      "metadata": {
        "id": "GbDnU2NXXdRG"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**terminal / bash komutu**"
      ],
      "metadata": {
        "id": "Hu7X2pD8XZH_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install -r requirements.txt"
      ],
      "metadata": {
        "id": "aYWrQFJSXbn1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "rN9syR6XTs-i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ead7d4d-6a85-4faf-9f93-bde91cf931cf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Gamze'nin en sevdiği renk: Kırmızı\n",
            "- Murat'ın en sevdiği renk: Mavi\n",
            "- Burak'ın en sevdiği renk: Turuncu\n"
          ]
        }
      ],
      "source": [
        "#%%writefile chain.py\n",
        "# First chain for RAG\n",
        "#Create Stuff Documents Chain\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.documents import Document # In RAG, we use that for input and output data of document. Document.page_content and Document.metadata\n",
        "from langchain_core.prompts import ChatPromptTemplate # It eliminate writing too long string\n",
        "from langchain.chains.combine_documents import create_stuff_documents_chain # It is the most significant method.\n",
        "\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "#load_dotenv()\n",
        "#my_key_openai = os.getenv(\"openai_apikey\")\n",
        "my_key_openai=\"---\"\n",
        "\n",
        "llm = ChatOpenAI(model=\"gpt-4-0125-preview\",api_key=my_key_openai)\n",
        "\n",
        "#ChatPromptTemplate creates string\n",
        "prompt= ChatPromptTemplate.from_messages(\n",
        "    [(\"system\", \"Burada ismi geçen kişilerin en sevdiği rengi tek tek yaz:\\n\\n{context}\")]\n",
        ")\n",
        "\n",
        "#above prompt code means that\n",
        "#prompt : f'''\"Burada ismi geçen kişilerin en sevdiği rengi tek tek yaz:\n",
        "#\n",
        "#\n",
        "#{context}\n",
        "#'''\n",
        "# also, we add System key.\n",
        "\n",
        "docs = [\n",
        "    Document(page_content=\"Gamze kırmızıyı sever ama sarıyı sevmez\"),\n",
        "    Document(page_content=\"Murat yeşili sever ama maviyi sevdiği kadar değil\"),\n",
        "    Document(page_content=\"Burak'a sorsan favori rengim yok der ama belli ki turuncu rengi seviyor\")\n",
        "] # our documents\n",
        "\n",
        "\n",
        "chain_1 = create_stuff_documents_chain(llm,prompt)\n",
        "print(chain_1.invoke({\"context\":docs})) # context is our docs.\n",
        "#chain_1 provides to create a chain among llm, prompt and docs\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Second chain\n",
        "#Create OpenAI Function Runnable Chain\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.prompts import ChatPromptTemplate # it provides to arrange prompt string\n",
        "from langchain_core.pydantic_v1 import BaseModel, Field # data formatting\n",
        "from typing import Optional # data formatting\n",
        "from langchain.chains.openai_functions import create_openai_fn_runnable #second chain\n",
        "\n",
        "#data types for insan\n",
        "class Insan(BaseModel): #İnsan sınıfı kalıtım yoluyla BaseModel sınıfından türetiliyor\n",
        "  \"\"\"Bir insan hakkında tanımlayıcı bilgiler\"\"\"\n",
        "  isim: str = Field(...,description=\"Kişinin İsmi\") # \"...\" means obligation\n",
        "  yas: int = Field(...,description=\"Kisinin yaşı\")\n",
        "  meslek: Optional[str] = Field(None,description=\"Kişinin Mesleği\")\n",
        "\n",
        "#data types for sehir\n",
        "class Sehir(BaseModel):\n",
        "  \"\"\"Bir şehir hakkında tanımlayıcı bilgiler\"\"\"\n",
        "\n",
        "  isim: str = Field(...,description=\"Şehrin İsmi\")\n",
        "  plaka_no: str = Field(...,description=\"Şehrin plaka numarası\")\n",
        "  iklim: Optional[str] = Field(None,description=\"Şehrin iklimi\")\n",
        "\n",
        "my_key_openai=\"---\"\n",
        "\n",
        "llm = ChatOpenAI(model=\"gpt-4-0125-preview\",api_key=my_key_openai)\n",
        "\n",
        "# we created prompt template.There are three messages which one is from system and the others are from human(user).\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\",\"Sen varlıkları kaydetmek konusunda dünyanın en başarılı algoritmasısın\"),\n",
        "        (\"human\",\"Şu verdiğim girdideki varlıkları kaydetmek için gerekli fonksiyonlara çağrı yap: {input}\"),\n",
        "        (\"human\",\"İpucu: Doğru formatta yanıtladığından emin ol\")\n",
        "    ]\n",
        ")\n",
        "chain_2 = create_openai_fn_runnable([Insan,Sehir],llm,prompt)\n",
        "print(chain_2.invoke({\"input\":\"Aydın 34 yaşında, başarılı bir bilgisayar mühendisiydi.\"}))\n",
        "print(chain_2.invoke({\"input\":\"Aydın'da hava her zaman sıcaktır ve bu yüzden 09 plakalı araçlarda klima hep çalışmaktadır.\"}))"
      ],
      "metadata": {
        "id": "mHrg05nv9-3-",
        "outputId": "a78a9bf1-4874-4af5-d52a-7a129f086535",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "isim='Aydın' yas=34 meslek='bilgisayar mühendisi'\n",
            "isim='Aydın' plaka_no='09' iklim='sıcak'\n"
          ]
        }
      ]
    }
  ]
}