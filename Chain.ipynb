{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPRgyjhoMJ5uYtrA0zBYjKZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kmk4444/Langchain/blob/main/Chain.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Langchain - Chains**\n",
        "- In Langchain, there are a lot of chains but we will examine two chains.\n",
        "- First chain relates on RAG.\n",
        "- Second chain relates on Function Calling which is answering our function."
      ],
      "metadata": {
        "id": "iU3vG17NV-zZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Requirements.txt**"
      ],
      "metadata": {
        "id": "WK_Q-1CCXWdK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!touch requirements.txt\n",
        "!echo langchain >> requirements.txt\n",
        "!echo langchain-openai >> requirements.txt\n",
        "!echo langchain-google-genai >> requirements.txt\n",
        "!echo langchain_experimental >> requirements.txt\n",
        "!echo openai >> requirements.txt\n",
        "!echo anthropic >> requirements.txt\n",
        "!echo cohere >> requirements.txt\n",
        "!echo streamlit >> requirements.txt\n",
        "!echo python-dotenv >> requirements.txt\n",
        "!echo beautifulsoup4 >> requirements.txt\n",
        "!echo faiss-cpu >> requirements.txt\n",
        "!echo pypdf >> requirements.txt\n",
        "!echo unstructured >> requirements.txt\n",
        "!echo networkx >> requirements.txt\n",
        "!echo openpyxl >> requirements.txt\n",
        "!echo rapidocr-onnxruntime >> requirements.txt"
      ],
      "metadata": {
        "id": "GbDnU2NXXdRG"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**terminal / bash komutu**"
      ],
      "metadata": {
        "id": "Hu7X2pD8XZH_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install -r requirements.txt"
      ],
      "metadata": {
        "id": "aYWrQFJSXbn1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "rN9syR6XTs-i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ead7d4d-6a85-4faf-9f93-bde91cf931cf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Gamze'nin en sevdiği renk: Kırmızı\n",
            "- Murat'ın en sevdiği renk: Mavi\n",
            "- Burak'ın en sevdiği renk: Turuncu\n"
          ]
        }
      ],
      "source": [
        "#%%writefile chain.py\n",
        "#Create Stuff Documents Chain\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.documents import Document # In RAG, we use that for input and output data of document. Document.page_content and Document.metadata\n",
        "from langchain_core.prompts import ChatPromptTemplate # It eliminate writing too long string\n",
        "from langchain.chains.combine_documents import create_stuff_documents_chain # It is the most significant method.\n",
        "\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "#load_dotenv()\n",
        "#my_key_openai = os.getenv(\"openai_apikey\")\n",
        "my_key_openai=\"---\"\n",
        "\n",
        "llm = ChatOpenAI(model=\"gpt-4-0125-preview\",api_key=my_key_openai)\n",
        "\n",
        "#ChatPromptTemplate creates string\n",
        "prompt= ChatPromptTemplate.from_messages(\n",
        "    [(\"system\", \"Burada ismi geçen kişilerin en sevdiği rengi tek tek yaz:\\n\\n{context}\")]\n",
        ")\n",
        "\n",
        "#above prompt code means that\n",
        "#prompt : f'''\"Burada ismi geçen kişilerin en sevdiği rengi tek tek yaz:\n",
        "#\n",
        "#\n",
        "#{context}\n",
        "#'''\n",
        "# also, we add System key.\n",
        "\n",
        "docs = [\n",
        "    Document(page_content=\"Gamze kırmızıyı sever ama sarıyı sevmez\"),\n",
        "    Document(page_content=\"Murat yeşili sever ama maviyi sevdiği kadar değil\"),\n",
        "    Document(page_content=\"Burak'a sorsan favori rengim yok der ama belli ki turuncu rengi seviyor\")\n",
        "] # our documents\n",
        "\n",
        "\n",
        "chain_1 = create_stuff_documents_chain(llm,prompt)\n",
        "print(chain_1.invoke({\"context\":docs})) # context is our docs.\n",
        "#chain_1 provides to create a chain among llm, prompt and docs\n",
        "\n"
      ]
    }
  ]
}