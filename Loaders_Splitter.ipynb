{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "https://github.com/kmk4444/Langchain/blob/main/Loaders_Splitter.ipynb",
      "authorship_tag": "ABX9TyMI3c44BdTYKkMMGalFoIfL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kmk4444/Langchain/blob/main/Loaders_Splitter.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Langchain provides more than one loaders but we focus on three loaders.\n",
        "1. WebBaseLoader - Load from a web page\n",
        "2. PyPDFLoader - Load data from a pdf\n",
        "3. UnstructuredExcelLoader - Load data from a excel\n",
        "\n",
        "Document loader means that several structure data converts into documents. (Document.page_content and Document.metada)"
      ],
      "metadata": {
        "id": "qI9MWHENTR0i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Requirements.txt**"
      ],
      "metadata": {
        "id": "qwhMJ04aZd9Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!touch requirements.txt\n",
        "!echo langchain >> requirements.txt\n",
        "!echo langchain-openai >> requirements.txt\n",
        "!echo langchain-google-genai >> requirements.txt\n",
        "!echo langchain_experimental >> requirements.txt\n",
        "!echo openai >> requirements.txt\n",
        "!echo anthropic >> requirements.txt\n",
        "!echo cohere >> requirements.txt\n",
        "!echo streamlit >> requirements.txt\n",
        "!echo python-dotenv >> requirements.txt\n",
        "!echo beautifulsoup4 >> requirements.txt\n",
        "!echo faiss-cpu >> requirements.txt\n",
        "!echo pypdf >> requirements.txt\n",
        "!echo unstructured >> requirements.txt\n",
        "!echo networkx >> requirements.txt\n",
        "!echo openpyxl >> requirements.txt\n",
        "!echo rapidocr-onnxruntime >> requirements.txt"
      ],
      "metadata": {
        "id": "o6GFm_r5Zev4"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**terminal / bash komutu**"
      ],
      "metadata": {
        "id": "o545Gy8eZg_p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install -r requirements.txt"
      ],
      "metadata": {
        "id": "_YFoCjcFZirj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "edROCO90STfY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b40f7073-193e-490f-9f73-997fb7e79d4e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dosya işlemi tamamlandı.\n",
            "{'source': 'https://kpmg.com/tr/tr/home/gorusler/2023/12/uretken-yapay-zeka-uygulamalarinin-kurumsallasma-yaklasimi.html', 'title': 'Üretken Yapay Zeka Uygulamalarının Kurumsallaşma Yaklaş - KPMG Türkiye', 'description': 'Mikro-Dil-Modeli Mimarisi', 'language': 'tr-TR'}\n",
            " \n",
            " \n",
            "1989: George Cybenko proves that neural networks can approximate continuous \n",
            "functions  \n",
            " \n",
            " \n",
            "1989: Yann LeCun's convolutional neural network for handwritten -digit recognition \n",
            "(LeNet -1)  \n",
            " \n",
            " \n",
            " \n",
            "1989: Kurt Hornik proves that neural networks are universal app roximators  \n",
            " \n",
            " \n",
            " \n",
            "1990: Carver Mead describes a neuromorph ic processor   {'source': '/content/drive/MyDrive/timeline.pdf', 'page': 39}\n",
            "##########################################################################################\n",
            "European Economy  Economic Briefs                                                                            Issue 054 | July 2020  \n",
            "  \n",
            " \n",
            " \n",
            "5 \n",
            " expansion of e -commerce and e -govern ment and the \n",
            "adoption of new  technology in the business sector.  \n",
            "The share of enterprises with hard -to-fill vacancies \n",
            "for ICT specialists rose from 3% in 2012 to 5% in \n",
            "2019  in EU28 .15 These skills shortages \n",
            "disproportionally affect larger companies (250 \n",
            "employees or more): the share o f large companies \n",
            "reporting such hard -to-fill vacancies rose from 18% \n",
            "in 2012 to 30% in 2019. Their incidence is \n",
            "(unsurprisingly) highest in the ICT sector (42%), \n",
            "and higher in professional services (9%) than in \n",
            "business support services (5%) and in the \n",
            "manufacturing sector (4%). Only 1% of companies \n",
            "with more than 10 employees report hard -to-fill \n",
            "vacancies for ICT specialists in the construction and \n",
            "in the accommodation and food service sectors.  \n",
            " \n",
            "Similar observations can be made with regard to \n",
            "other skills  types. Studies based on PIAAC data \n",
            "show that while some EU Member States are among \n",
            "the top of the global league in terms of average \n",
            "numeracy and literacy skills, other s lag considerably \n",
            "behind. W ithin countries, there is even more \n",
            "variation in skills prof iciency  than between \n",
            "countries.  Around 20% of the European population \n",
            "at working age lacks the level of numeracy and \n",
            "literacy skills that is consi dered essential for \n",
            "successful participation in the economy and in \n",
            "society overall (European Commission, 2013 a and \n",
            "Graph 2 ). Less is known about how European adults \n",
            "perform in terms of non -cognitive skills.   \n",
            " \n",
            "The OECD’s PIAAC dataset allows explor ing the \n",
            "variation in different types of skills across 20 \n",
            "countries and a range of sectors in the EU. While the \n",
            "dataset  also covers inactive and unemployed \n",
            "individuals, for the remainder of this paper we will \n",
            "focus on the skills of working individuals only, as \n",
            "we will then relate these skills to economic \n",
            "performance as measured by labour  productivity.  \n",
            " \n",
            "We consider cognitive foundation skills (literacy, \n",
            "numeracy and problem solving), technical digital  \n",
            "skills (simple and more complex ones), and a set of \n",
            "non-cognitive skills (self -organi sation, interaction \n",
            "and communication; managing and supervision; \n",
            "readin ess to learn and creativity; trust in persons; \n",
            "conscientiousness). Lastly, we also consider physical \n",
            "skills. Only the cognitive foundation skills are \n",
            "measured directly; the other skills are assessed based \n",
            "on the extent to which they are used on the job as a \n",
            "proxy. Our skills typology is strongly inspired by \n",
            "earlier work based on PIAAC by Grundke et al. (2016) who present a normative taxonomy of skills \n",
            "that can be operationali sed using data from the \n",
            "PIAAC survey (see Box 1).  \n",
            "Graph 2:  Upskilling challenges E urope faces  \n",
            " \n",
            "Source:  Digital Economy and Society Index (DESI) 2017 \n",
            "and PIAAC data 2013 . \n",
            " \n",
            "Graph s 3-5 provide a first intuition in the variation \n",
            "in skills levels (cognitive, non-cognitive , physical \n",
            "skills)  across countries and (EU -level aggregated) \n",
            "sectors.  The data suggest variation in workers’ skills \n",
            "levels across countries. For cognitive skills, the \n",
            "Netherlands, Belgium (Flanders) and Estonia stand \n",
            "out as best performers; but Slovakia, Czechia, \n",
            "Finland, Hungary and Denmark perform well , too. \n",
            "At the other end, Greece, Cyprus, Spain and France \n",
            "perform rather weakly. There is no clear relationship \n",
            "emerging between cognitive skills and other types of \n",
            "skills at the country level. Also in statistical terms, \n",
            "there is no significant correlation. The best \n",
            "performers in terms of non -cognitive skills use are \n",
            "Denmark, Finland and Sweden, but also Poland, \n",
            "Ireland and Spain perform relatively well. On the \n",
            "other hand, Greece, Lithuania and Cyprus perform \n",
            "weakly at this l evel – making Greece and Cyprus \n",
            "bottom performers both in cognitive and non -\n",
            "cognitive skills among the EU countries covered by \n",
            "PIAAC. Physical skills are used most often in \n",
            "Slovenia and Lithuania, Ireland and Poland; and \n",
            "least often in Hungary, France, Fin land and Belgium \n",
            "(Flanders).   \n",
            "MORETHAN\n",
            "4IN10\n",
            "EUROPEANS\n",
            "LACKBASICDIGITALSKILLS\n",
            "2IN5\n",
            "EUROPEANS\n",
            "LACKADEQUATEREADING\n",
            "ANDWRITINGSKILLS\n",
            "##########################################################################################\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dosya işlemi tamamlandı.\n"
          ]
        }
      ],
      "source": [
        "# LOADER\n",
        "\n",
        "# WebBaseLoader\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "\n",
        "target_url=\"https://kpmg.com/tr/tr/home/gorusler/2023/12/uretken-yapay-zeka-uygulamalarinin-kurumsallasma-yaklasimi.html\"\n",
        "\n",
        "loader = WebBaseLoader(target_url) # we create a loader object and we assign to this object.\n",
        "raw_documents = loader.load() # we load document from url. (type is document)\n",
        "\n",
        "\n",
        "with open(\"URL_Icerik.txt\",\"w\") as file: # we create a file to save raw_documents for string format.\n",
        "  file.write(raw_documents[0].page_content) #page_content is information we want.\n",
        "\n",
        "print(\"Dosya işlemi tamamlandı.\")\n",
        "\n",
        "print(raw_documents[0].metadata) # we can design metadata in RAG.\n",
        "\n",
        "# PyPDFLoader - Bir PDF dosyasından içerik yükleme\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "\n",
        "#first file\n",
        "filepath = \"/content/drive/MyDrive/timeline.pdf\"\n",
        "loader = PyPDFLoader(filepath) # we are ready to load our file\n",
        "pages = loader.load() # we load our pdf\n",
        "\n",
        "print(pages[39].page_content,pages[39].metadata) # index 39 means that 40th page\n",
        "##########################################################################################\n",
        "print(\"##########################################################################################\")\n",
        "#second file\n",
        "filepath = \"/content/drive/MyDrive/digital.pdf\"\n",
        "loader = PyPDFLoader(filepath,extract_images=True) # extract_images provides that could we convert image to text? We are checking.\n",
        "pages = loader.load()# we load our pdf\n",
        "print(pages[6].page_content)\n",
        "##########################################################################################\n",
        "print(\"##########################################################################################\")\n",
        "\n",
        "#UnstructuredExcelLoader - Load data from a excel\n",
        "from langchain_community.document_loaders import UnstructuredExcelLoader\n",
        "filepath=\"/content/drive/MyDrive/ai_course.xlsx\"\n",
        "loader = UnstructuredExcelLoader(filepath, mode=\"elements\") # it converts excel table into html table\n",
        "docs = loader.load()\n",
        "\n",
        "excel_content = docs[0].metadata[\"text_as_html\"] # all information of excel is 0th index.\n",
        "\n",
        "# we will save html data in html file.\n",
        "with open(\"excel.html\",\"w\") as file:\n",
        "  file.write(excel_content)\n",
        "\n",
        "print(\"Dosya işlemi tamamlandı.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Resource:**\n",
        "- https://github.com/FullStackRetrieval-com/RetrievalTutorials/blob/main/tutorials/LevelsOfTextSplitting/5_Levels_Of_Text_Splitting.ipynb"
      ],
      "metadata": {
        "id": "oRhHSioZuXJo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "# SPLITTER\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter\n",
        "from langchain_experimental.text_splitter import SemanticChunker\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "#load_dotenv()\n",
        "#my_key_openai = os.getenv(\"openai_apikey\")\n",
        "#my_key_google = os.getenv(\"google_apikey\")\n",
        "my_key_openai=\"----\"\n",
        "my_key_google=\"----\"\n",
        "\n",
        "llm_gemini = ChatGoogleGenerativeAI(google_api_key=my_key_google, model=\"gemini-pro\", convert_system_message_to_human=True)\n",
        "\n",
        "embeddings = OpenAIEmbeddings(api_key=my_key_openai)\n",
        "\n",
        "def split_content(splitter_type, target_url=\"\", chunk_size=500, chunk_overlap=0):\n",
        "  loader = WebBaseLoader(target_url)\n",
        "  raw_documents = loader.load()\n",
        "\n",
        "  if splitter_type == \"Character\": # it depends on the number of character. Sometimes, to save content, character number can be increased. It is langchain advantage :)\n",
        "    text_splitter = CharacterTextSplitter(\n",
        "        chunk_size=chunk_size, # chunk_size means that the number of character.\n",
        "        chunk_overlap=chunk_overlap, # for example, you selected chunk_size 800. First chunk consists of 800 character and you selected that chunk_overlap is 200.\n",
        "        #Second chunk will start the last 200 character of first chumk and 600 following characters.\n",
        "        length_function=len\n",
        "    )\n",
        "\n",
        "  elif splitter_type == \"Recursive\": # It depends on number of character and recursive characters(for example line space, comma, dot).\n",
        "    # It seperates chunk using these recursive characters. Also, you can determine character.\n",
        "    text_splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=chunk_size,\n",
        "        chunk_overlap=chunk_overlap,\n",
        "        length_function=len\n",
        "    )\n",
        "\n",
        "\n",
        "  elif splitter_type == \"Semantic\": # It just depends on context. It seperates chunks using semantic algorithm.\n",
        "    text_splitter = SemanticChunker(embeddings)\n",
        "\n",
        "  splitted_documents = text_splitter.split_documents(raw_documents)\n",
        "\n",
        "  return splitted_documents\n",
        "\n",
        "import streamlit as st\n",
        "\n",
        "st.set_page_config(page_title=\"Splitter Karşılaştırması\", layout=\"wide\")\n",
        "st.title(\"Splitter Karşılaştırılması\")\n",
        "st.divider()\n",
        "\n",
        "target_url = st.text_input(label=\"İşlenecek Web Adresini Giriniz:\")\n",
        "st.divider()\n",
        "chunk_size = st.slider(label=\"Kesit büyüklüğünü belirleyiniz:\",min_value=100, max_value=2000, value=1000, step=100, key=\"url_chunk_size\")\n",
        "st.divider()\n",
        "chunk_overlap = st.slider(label=\"Çakışma büyüklüğünü belirleyiniz\",min_value=0, max_value=1000, value=0, step=100, key=\"url_chunk_overlap\")\n",
        "st.divider()\n",
        "submit_btn= st.button(label=\"Başla\", key = \"url_button\")\n",
        "st.divider()\n",
        "\n",
        "if submit_btn:\n",
        "  col_character, col_recursive, col_semantic = st.columns(3)\n",
        "\n",
        "  with col_character:\n",
        "    splitted_documents = split_content(splitter_type=\"Character\", target_url=target_url, chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
        "    st.subheader(\"Character Splitter\")\n",
        "    for splitted_document in splitted_documents:\n",
        "      st.success(splitted_document.page_content)\n",
        "\n",
        "  with col_recursive:\n",
        "    splitted_documents = split_content(splitter_type=\"Recursive\", target_url=target_url, chunk_size=chunk_size,chunk_overlap=chunk_overlap)\n",
        "    st.subheader(\"Recursive Character Splitter\")\n",
        "    for splitted_document in splitted_documents:\n",
        "      st.info(splitted_document.page_content)\n",
        "\n",
        "  with col_semantic:\n",
        "    splitted_documents = split_content(splitter_type=\"Semantic\",target_url=target_url,chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
        "    st.subheader(\"Semantic Splitter\")\n",
        "    for splitted_document in splitted_documents:\n",
        "      st.warning(splitted_document.page_content)\n",
        "\n"
      ],
      "metadata": {
        "id": "S5xy2MOnq6h0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "95b2914a-7739-42d9-c6d2-3aea45d073bc"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!npm install localtunnel\n",
        "!streamlit run /content/app.py &>/content/logs.txt &\n",
        "!npx localtunnel --port 8501"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rJAuHTBNqor8",
        "outputId": "124e8334-bcd0-48f4-f1eb-2cc9dbf3e2fc"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K\u001b[?25h\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m \u001b[0m\u001b[35msaveError\u001b[0m ENOENT: no such file or directory, open '/content/package.json'\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m \u001b[0m\u001b[35menoent\u001b[0m ENOENT: no such file or directory, open '/content/package.json'\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No description\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No repository field.\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No README data\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No license field.\n",
            "\u001b[0m\n",
            "\u001b[K\u001b[?25h+ localtunnel@2.0.2\n",
            "updated 1 package and audited 36 packages in 0.436s\n",
            "\n",
            "3 packages are looking for funding\n",
            "  run `npm fund` for details\n",
            "\n",
            "found 2 \u001b[93mmoderate\u001b[0m severity vulnerabilities\n",
            "  run `npm audit fix` to fix them, or `npm audit` for details\n",
            "\u001b[K\u001b[?25hnpx: installed 22 in 1.918s\n",
            "your url is: https://lucky-tools-end.loca.lt\n",
            "^C\n"
          ]
        }
      ]
    }
  ]
}